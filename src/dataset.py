# src/dataset.py (å·²ä¿®æ”¹ï¼šæ”¯æŒåŠ è½½ F_s_e2v æˆ– F_s_wavlm)

import torch
from torch.utils.data import Dataset
import os
import re # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼è§£ææ–‡ä»¶
import numpy as np 

# å®šä¹‰æƒ…ç»ªæ ‡ç­¾åˆ° ID çš„æ˜ å°„
EMO_MAP = {
    'hap': 0, 'exc': 0, # ç»Ÿä¸€åˆ° Happy/Excited (ID 0)
    'ang': 1, # Angry (ID 1)
    'sad': 2, # Sad (ID 2)
    'neu': 3, # Neutral (ID 3)
}
TARGET_EMOS = list(EMO_MAP.keys())


class IEMOCAPDataset(Dataset):
    """
    å¤„ç† IEMOCAP æ•°æ®é›†ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸º EPC ä»»åŠ¡çš„è¾“å…¥ã€‚
    """
    
    def __init__(self, data_root, target_session, is_train=True, history_len=3, 
                 feature_cache_path=None, speech_feature_tag=None): # <-- å…³é”®æ–°å¢ tag å‚æ•°
        """
        åˆå§‹åŒ–æ•°æ®é›†ã€‚
        :param feature_cache_path: åªæœ‰åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µæ‰ä¼ å…¥ã€‚
        :param speech_feature_tag: 'e2v' æˆ– 'wavlm'ã€‚å†³å®šåŠ è½½å“ªå¥—è¯­éŸ³ç‰¹å¾ã€‚
        """
        self.data_root = data_root
        self.history_len = history_len
        self.feature_cache_path = feature_cache_path
        self.speech_feature_tag = speech_feature_tag # <-- ä¿å­˜ tag
        
        # --- å…³é”®ä¿®æ”¹ 1ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç¼“å­˜æ¨¡å¼ ---
        self.is_cached_mode = (feature_cache_path is not None)
        self.cached_features = {} 
        
        # é¢„åŠ è½½æ‰€æœ‰ Session çš„æ•°æ® (è·å– ID, Label, é¡ºåº)
        self.raw_utterances_by_session = {}
        for i in range(1, 6):
            session_name = f"Session{i}"
            self.raw_utterances_by_session[session_name] = self._collect_session_utterances(session_name)
        
        # --- å…³é”®ä¿®æ”¹ 2ï¼šåœ¨ç¼“å­˜æ¨¡å¼ä¸‹é¢„åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶ ---
        if self.is_cached_mode:
             self._load_all_cached_features()
             print(f"ğŸ” DEBUG | Cached sessions detected: {list(self.cached_features.keys())}")

        # åŸºäº LOO-CV è§„åˆ™åˆ‡åˆ†æ ·æœ¬
        self.samples = self._load_and_split_data(target_session, is_train)
        print(f"Loaded {'Train' if is_train else 'Test'} samples: {len(self.samples)} for target session {target_session}")

    # --- æ–°å¢è¾…åŠ©æ–¹æ³• 1ï¼šåŠ è½½æ‰€æœ‰ç¼“å­˜ç‰¹å¾æ–‡ä»¶ (æ”¯æŒ tag åŒºåˆ†) ---
    def _load_all_cached_features(self):
        if self.speech_feature_tag not in ['e2v', 'wavlm']:
            raise ValueError("speech_feature_tag must be 'e2v' or 'wavlm' in cached mode.")
            
        tag = self.speech_feature_tag

        for i in range(1, 6):
            session = f"Session{i}"
            ft_path = os.path.join(self.feature_cache_path, f'{session}_F_t.npy')
            fs_path = os.path.join(self.feature_cache_path, f'{session}_F_s_{tag}.npy')
            ids_path = os.path.join(self.feature_cache_path, f'{session}_utt_ids.npy')

            # âœ… å¿…é¡»ç¡®ä¿ä¸‰è€…éƒ½å­˜åœ¨å†åŠ è½½
            if not all(os.path.exists(p) for p in [ft_path, fs_path, ids_path]):
                print(f"âš ï¸ Skipping {session}: Missing one or more required feature files.")
                continue

            try:
                F_t = np.load(ft_path)
                F_s = np.load(fs_path)
                utt_ids = np.load(ids_path, allow_pickle=True)
            except Exception as e:
                print(f"âŒ Error loading cached features for {session}: {e}")
                continue

            self.cached_features[session] = {
                'F_t': F_t,
                'F_s': F_s,
                'id_to_index': {id: i for i, id in enumerate(utt_ids)}
            }

        print(f"âœ… Cached sessions loaded: {list(self.cached_features.keys())}")

    # --- _load_and_split_data ä¿æŒä¸å˜ (å®ƒåªç®¡æ•°æ®åˆ‡åˆ†å’Œæ ‡ç­¾) ---
    def _load_and_split_data(self, target_session, is_train):
        # ... (ä¿æŒä¸å˜) ...
        # æ³¨æ„: è¿™é‡Œéœ€è¦æ·»åŠ ä½ ä¹‹å‰åœ¨ discussion ä¸­ç¡®è®¤çš„ 'session', 'history_utt_ids', 'target_utt_id' å­—æ®µ
        
        all_samples = []
        sessions_all = [f"Session{i}" for i in range(1, 6)]
        
        data_sessions = [s for s in sessions_all if s != target_session] if is_train else [target_session]
        
        print(f"Processing sessions: {data_sessions}")

        for session in data_sessions:
            utterances = self.raw_utterances_by_session[session]
            if not utterances:
                continue

            for i in range(len(utterances)):
                if i + 1 < len(utterances):
                    
                    target_utterance = utterances[i + 1]
                    target_emo_str = target_utterance['label']
                    
                    if target_emo_str in TARGET_EMOS:
                        history_start_index = max(0, i - self.history_len + 1)
                        history = utterances[history_start_index : i + 1]
                        target_label_id = EMO_MAP[target_emo_str]
                        
                        sample = {
                            'session': session, 
                            'history_utt_ids': [u['utt_id'] for u in history], 
                            'target_utt_id': target_utterance['utt_id'], 
                            
                            'history_texts': [u['text'] for u in history],
                            'history_audio_paths': [u['audio_path'] for u in history],
                            'target_label': target_label_id,
                            'target_text': target_utterance['text'], 
                            'target_audio_path': target_utterance['audio_path'] 
                        }
                        all_samples.append(sample)

        return all_samples
        

    def _collect_session_utterances(self, session):
        """
        è§£æ IEMOCAP åŸå§‹æ–‡ä»¶ï¼Œæ”¶é›†ä¸€ä¸ª Session å†…æ‰€æœ‰å›åˆçš„æ–‡æœ¬ã€æ ‡ç­¾å’ŒéŸ³é¢‘è·¯å¾„ã€‚
        """
        
        # 1. æ‰¾åˆ°è¯¥ Session ä¸‹çš„å¯¹è¯ç›®å½• (è½¬å½•)
        session_dir = os.path.join(self.data_root, session, 'dialog', 'transcriptions')
        
        # DEBUG: æ£€æŸ¥è·¯å¾„
        # print(f"DEBUG: Checking transcription dir: {session_dir}") 
        
        if not os.path.exists(session_dir):
            print(f"ERROR: Transcription directory not found: {session_dir}.")
            return [] 

        # 2. éå†è½¬å½•æ–‡ä»¶ä»¥è·å– Utterance ID, æ–‡æœ¬å’Œæ—¶é—´é¡ºåº
        dialog_trans_files = [f for f in os.listdir(session_dir) if f.endswith('.txt')]
        
        dialog_data = {} 
        
        # ã€è½¬å½•æ­£åˆ™ã€‘åŒ¹é…ï¼šUtteranceID [TIME_START-TIME_END]: Text
        trans_regex_full = re.compile(r'(\w+)\s*\[([\d\.]+)-([\d\.]+)]:\s*(.*)', re.M)

        for trans_file_name in dialog_trans_files:
            trans_path = os.path.join(session_dir, trans_file_name)
            content = ""
            
            try:
                with open(trans_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                with open(trans_path, 'r', encoding='latin-1') as f:
                    content = f.read()

            matches = trans_regex_full.findall(content)

            for utt_id, start_time, end_time, text_raw in matches:
                text = text_raw.strip().replace('\n', ' ')
                dialog_data[utt_id] = {
                    'text': text,
                    'start': float(start_time), 
                    'end': float(end_time),
                }

        # 3. éå†æƒ…ç»ªæ ‡æ³¨æ–‡ä»¶æ¥æ·»åŠ æƒ…ç»ªæ ‡ç­¾å’Œæ—¶é—´é¡ºåº
        emotion_dir = os.path.join(self.data_root, session, 'dialog', 'EmoEvaluation')
        
        if not os.path.exists(emotion_dir):
            print(f"ERROR: Emotion directory not found: {emotion_dir}.")
            return [] 
            
        dialog_emo_files = [f for f in os.listdir(emotion_dir) if f.endswith('.txt')]
        
        # ã€æœ€ç»ˆä¸”æœ€å®½æ¾çš„æƒ…ç»ªæ­£åˆ™ã€‘
        emo_regex = re.compile(
            r'\[.+?\]\s+'
            r'([\w\-]+)\s+'
            r'([A-Za-z]+)',
            re.IGNORECASE
        )

        final_utterance_list = [] # æœ€ç»ˆæŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„å›åˆåˆ—è¡¨

        for emo_file_name in dialog_emo_files:
            emo_path = os.path.join(emotion_dir, emo_file_name)
            content = ""
            
            try:
                with open(emo_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                with open(emo_path, 'r', encoding='latin-1') as f:
                    content = f.read()
                
            # æ‰¾åˆ°æ‰€æœ‰æƒ…ç»ªæ ‡æ³¨åŒ¹é…é¡¹
            matches = emo_regex.findall(content)
            
            for utt_id, label in matches:
                label = label.lower()
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æˆ‘ä»¬å·²ç»è§£æçš„è½¬å½•æ–‡æœ¬ä¸­
                if utt_id in dialog_data:
                    data = dialog_data[utt_id]
                    dialog_name = "_".join(utt_id.split('_')[:-1])
                    audio_sub_path = os.path.join(
                        session, 'sentences', 'wav', dialog_name, f'{utt_id}.wav'
                    )
                    full_audio_path = os.path.join(self.data_root, audio_sub_path)
                    
                    if label in TARGET_EMOS or label in ['fru', 'sur', 'dis', 'oth', 'xxx']:
                        final_utterance_list.append({
                            'utt_id': utt_id, 'text': data['text'], 'audio_path': full_audio_path,
                            'label': label, 'start': data['start'], 'end': data['end']
                        })


        # 4. æŒ‰ utt_id æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
        final_utterance_list.sort(key=lambda x: x['utt_id'])
        
        return final_utterance_list


    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        if self.is_cached_mode:
            # --- ç¼“å­˜æ¨¡å¼ï¼šè¿”å›ç‰¹å¾å¼ é‡å’Œæ ‡ç­¾ ---
            session = sample['session']
            if session not in self.cached_features:
                raise RuntimeError(f"Missing cached features for {session}. Check if {self.feature_cache_path} is correct.")
            
            cached_data = self.cached_features[session]
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰å›åˆçš„ç‰¹å¾
            all_utt_ids = sample['history_utt_ids'] + [sample['target_utt_id']]
            F_t_sequence_list = []
            F_s_sequence_list = []
            
            for utt_id in all_utt_ids:
                if utt_id not in cached_data['id_to_index']:
                    raise IndexError("Missing ID in cache map.") 
                    
                index = cached_data['id_to_index'][utt_id]
                F_t_sequence_list.append(cached_data['F_t'][index])
                F_s_sequence_list.append(cached_data['F_s'][index])

            # 2. å †å å¹¶è½¬æ¢ä¸º Tensor
            F_t_sequence = torch.tensor(np.stack(F_t_sequence_list, axis=0), dtype=torch.float32)
            F_s_sequence = torch.tensor(np.stack(F_s_sequence_list, axis=0), dtype=torch.float32)
            
            # æ ‡ç­¾
            target_label = torch.tensor(sample['target_label'], dtype=torch.long)
            
            # åºåˆ—é•¿åº¦
            seq_len = F_t_sequence.shape[0]

            return {
                'F_t': F_t_sequence,
                'F_s': F_s_sequence, 
                'target_label': target_label,
                'mask': torch.ones(seq_len, dtype=torch.bool)
            }


        else:
            # --- åŸå§‹é€»è¾‘ (ç‰¹å¾æå–é˜¶æ®µ): è¿”å›æ–‡æœ¬/éŸ³é¢‘è·¯å¾„ ---
            return {
                'history_texts': sample['history_texts'],
                'history_audio_paths': sample['history_audio_paths'],
                'target_text': sample['target_text'],
                'target_audio_path': sample['target_audio_path'],
                'target_label': sample['target_label']
            }

# ====================================================================
# æœ¬åœ°è°ƒè¯•ä»£ç  (IEMOCAP æ•°æ®ä¸‹è½½å®Œæˆåï¼Œå†è¿è¡Œ)
# ====================================================================
if __name__ == '__main__':
    print("--- Testing IEMOCAP Dataset Initialization (Requires actual data structure) ---")
    
    # å‡è®¾è¿™é‡Œçš„è·¯å¾„æ˜¯åŸå§‹æ•°æ®è·¯å¾„
    IEMOCAP_ROOT = 'YOUR_IEMOCAP_ROOT_PATH' 
    # å‡è®¾ç¼“å­˜ç‰¹å¾è·¯å¾„
    CACHE_ROOT = 'YOUR_CACHE_ROOT_PATH' 

    try:
        # æ¨¡å¼ 1: ç‰¹å¾æå–æ¨¡å¼ (æ—  cache path)
        print("\n--- Testing Feature Extraction Mode (Original Data) ---")
        train_dataset_raw = IEMOCAPDataset(IEMOCAP_ROOT, target_session='Session5', is_train=True, history_len=3)
        print(f"Train Dataset Size (Raw Mode): {len(train_dataset_raw)}")
        # æ£€æŸ¥ __getitem__ è¿”å›åŸå§‹è·¯å¾„
        sample_raw = train_dataset_raw[0]
        print(f"Raw Sample Return: Text={sample_raw['history_texts'][0][:15]}..., Audio Path={sample_raw['history_audio_paths'][0]}")

        # æ¨¡å¼ 2: æ¨¡å‹è®­ç»ƒæ¨¡å¼ (æœ‰ cache path)
        print("\n--- Testing Model Training Mode (Cached Features) ---")
        # æ³¨æ„: è¿è¡Œæ­¤æ¨¡å¼éœ€è¦ CACHE_ROOT çœŸå®å­˜åœ¨ä¸”åŒ…å«ç‰¹å¾æ–‡ä»¶
        
        # ğŸš¨ ä¿®æ­£ï¼šå¿…é¡»ä¼ å…¥ speech_feature_tag å‚æ•°
        train_dataset_cached = IEMOCAPDataset(
            IEMOCAP_ROOT, 
            target_session='Session5', 
            is_train=True, 
            history_len=3, 
            feature_cache_path=CACHE_ROOT,
            speech_feature_tag='e2v' # <-- å‡è®¾æˆ‘ä»¬åŠ è½½ e2v ç‰¹å¾è¿›è¡Œæµ‹è¯•
        )
        print(f"Train Dataset Size (Cached Mode): {len(train_dataset_cached)}")
        # æ£€æŸ¥ __getitem__ è¿”å›ç‰¹å¾å¼ é‡
        sample_cached = train_dataset_cached[0]
        print(f"Cached Sample Return: F_t Shape={sample_cached['F_t'].shape}, Label={sample_cached['target_label']}")
        
    except Exception as e:
        print(f"\nFATAL ERROR: Dataset testing failed. Ensure paths and data exist.")
        print(f"Details: {e}")