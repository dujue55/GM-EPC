# src/trainer.py (æœ€ç»ˆä¿®æ­£ç‰ˆï¼šå¥å£®ã€ä¸¥è°¨ã€ç¬¦åˆå­¦æœ¯è§„èŒƒ)

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
# ğŸš¨ ä¿®æ­£ 7ï¼šä½¿ç”¨æ™ºèƒ½ tqdm å¯¼å…¥ï¼Œå…¼å®¹ Notebook å’Œå‘½ä»¤è¡Œ
from tqdm.auto import tqdm 
from sklearn.metrics import f1_score, recall_score # ä¿®æ­£ 6ï¼šå¯¼å…¥ recall_score ç”¨äº UAR
import os
import numpy as np
import pandas as pd
import time
import copy 
import sys 

# --- ä»å…¶ä»–æ¨¡å—å¯¼å…¥å¿…è¦çš„ç»„ä»¶ ---
from model import GatedMultimodalEPC, TextOnlyModel, SpeechOnlyModel, StaticFusionModel, BaseWavLMModel 
# å‡è®¾ features.py ä¸­çš„ get_dummy_features ç°åœ¨è¿”å› F_t, F_s_e2v, F_s_wavlm (ä¸‰ä¸ª)
from features import get_dummy_features, get_dummy_labels, TEXT_DIM, SPEECH_DIM 
from dataset import IEMOCAPDataset 


# --- å ä½ç¬¦ï¼šè™šæ‹Ÿæ•°æ®é›† (ç”¨äºæœ¬åœ°æµ‹è¯•) ---
class DummyConversationDataset(Dataset):
    # ... (åˆå§‹åŒ–å’Œ __len__ ä¿æŒä¸å˜) ...
    def __init__(self, num_samples, history_len, num_classes):
        self.num_samples = num_samples
        self.history_len = history_len
        self.num_classes = num_classes
        
    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # ğŸš¨ ä¿®æ­£ï¼šä» features å¯¼å…¥çš„ get_dummy_features ç°åœ¨è¿”å› F_t, F_s_e2v, F_s_wavlm (ä¸‰ä¸ª)
        F_t, F_s_e2v, F_s_wavlm = get_dummy_features(batch_size=1, sequence_length=self.history_len)
        labels = get_dummy_labels(batch_size=1, num_classes=self.num_classes)
        
        # é»˜è®¤ä½¿ç”¨ F_s_e2v ä½œä¸ºé€šç”¨ F_s è¿›è¡Œæµ‹è¯•
        return {
            'F_t': F_t.squeeze(0),
            'F_s': F_s_e2v.squeeze(0), 
            'target_label': labels.squeeze(0), 
            'mask': torch.ones(self.history_len, dtype=torch.bool)
        }


class Trainer:
    # ... (__init__ ä¿æŒä¸å˜) ...
    def __init__(self, model, learning_rate, weight_decay, num_classes, patience=10):
        self.model = model
        self.criterion = nn.CrossEntropyLoss() 
        self.optimizer = AdamW(
            self.model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        self.num_classes = num_classes
        self.patience = patience 
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        # print(f"Trainer initialized. Using device: {self.device}") # ç®€åŒ–è¾“å‡º

    def train_epoch(self, dataloader, epoch_idx):
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []

        desc = f"Epoch {epoch_idx + 1:02d} | Train"
        for batch in tqdm(dataloader, desc=desc): 
            # ... (è®­ç»ƒé€»è¾‘ä¿æŒä¸å˜) ...
            F_t = batch['F_t'].to(self.device)
            F_s = batch['F_s'].to(self.device)
            labels = batch['target_label'].to(self.device)

            self.optimizer.zero_grad()
            
            model_output = self.model(F_t, F_s)
            
            if isinstance(model_output, tuple):
                logits = model_output[0]
            else:
                logits = model_output
            
            if labels.dim() > 1:
                labels = labels.squeeze(-1)

            loss = self.criterion(logits, labels)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item() * F_t.size(0)
            
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
        avg_loss = total_loss / len(dataloader.dataset)
        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
        
        return avg_loss, macro_f1


    def evaluate(self, dataloader, desc="Evaluation"):
        self.model.eval() 
        total_loss = 0
        all_preds = []
        all_labels = []
        all_gate_weights = [] 

        with torch.no_grad():
            for batch in tqdm(dataloader, desc=desc): 
                F_t = batch['F_t'].to(self.device)
                F_s = batch['F_s'].to(self.device)
                labels = batch['target_label'].to(self.device)
                
                if labels.dim() > 1:
                    labels = labels.squeeze(-1)

                model_output = self.model(F_t, F_s)
                
                if isinstance(model_output, tuple): 
                    logits = model_output[0]
                    gate_weights = model_output[1] 
                    
                    # æ”¶é›†æœ€åä¸€ä¸ªå›åˆçš„å¹³å‡æƒé‡
                    avg_gate_per_sample = gate_weights[:, -1, :].mean(dim=-1).cpu().numpy()
                    all_gate_weights.extend(avg_gate_per_sample)
                else:
                    logits = model_output 
                
                loss = self.criterion(logits, labels)
                total_loss += loss.item() * F_t.size(0)
                
                preds = torch.argmax(logits, dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        avg_loss = total_loss / len(dataloader.dataset)
        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
        
        # ğŸš¨ ä¿®æ­£ 6ï¼šæ­£ç¡®è®¡ç®— UAR (Unweighted Average Recall / Macro Recall)
        uar = recall_score(all_labels, all_preds, average='macro', zero_division=0) 
        
        return avg_loss, macro_f1, uar, all_labels, all_preds, all_gate_weights


# --- å¤–éƒ¨è¿è¡Œå‡½æ•° (run_cross_validation) ---

# ğŸš¨ ä¿®æ­£ 3ï¼šç§»é™¤æœªä½¿ç”¨çš„ cv_data_split å‚æ•°
def run_cross_validation(ModelClass, config):
    
    # ğŸš¨ ä¿®æ­£ 2ï¼šæ£€æŸ¥é…ç½®é”®åæ˜¯å¦åŒ¹é…
    if 'original_data_root' not in config:
        config['original_data_root'] = config.get('data_root', '/path/to/iemocap') # é€‚é…æœ¬åœ°æµ‹è¯•
    
    # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šä½¿ç”¨çš„è¯­éŸ³ç‰¹å¾ tag
    model_name = ModelClass.__name__
    if model_name in ['GatedMultimodalEPC', 'SpeechOnlyModel', 'StaticFusionModel', 'TextOnlyModel']:
        tag = 'e2v' 
    elif model_name == 'BaseWavLMModel':
        tag = 'wavlm'
    else:
        raise ValueError(f"Unknown Model Class {model_name} for tag determination.")


    sessions = [f'Session{i}' for i in range(1, 6)] 
    
    # ğŸš¨ ä¿®æ­£ 1ï¼šæœªå®šä¹‰å˜é‡çš„åˆå§‹åŒ–
    # åˆå§‹åŒ–ç»“æœ DataFrame
    results_df = pd.DataFrame(columns=['Session', 'Test_Loss', 'Test_Macro_F1', 'Test_UAR', 'Train_Time_s', 'Best_Epoch', 'Params (M)'])
    all_test_f1s = []
    
    # åˆå§‹åŒ–å…¨å±€æ•°æ®æ”¶é›†åˆ—è¡¨
    global_labels = []
    global_preds = []
    global_gate_weights = []
    
    print(f"\n--- Starting 5-Fold Cross-Validation for {ModelClass.__name__} (Feature: {tag}) ---")

    for fold_idx, target_session in enumerate(sessions):
        # ğŸš¨ ä¿®æ­£ 4ï¼šè®­ç»ƒæ—¶é—´ç»Ÿè®¡åº”åœ¨ fold å†…éƒ¨
        start_time = time.time()
        
        print(f"\n=======================================================")
        print(f"| FOLD {fold_idx + 1}/5: Test on {target_session} |")
        print(f"=======================================================")
        

        # --- 1. æ•°æ®åŠ è½½ (ä¼ å…¥ tag) ---
        
        train_dataset = IEMOCAPDataset(
            config['original_data_root'], 
            target_session, 
            is_train=True, 
            history_len=config['history_len'], 
            feature_cache_path=config['feature_cache_path'],
            speech_feature_tag=tag 
        )
        test_dataset = IEMOCAPDataset(
            config['original_data_root'], 
            target_session, 
            is_train=False, 
            history_len=config['history_len'], 
            feature_cache_path=config['feature_cache_path'],
            speech_feature_tag=tag 
        )

        # ğŸš¨ æ›¿æ¢ï¼šä½¿ç”¨è™šæ‹Ÿæ•°æ®åŠ è½½å™¨è¿›è¡Œæœ¬åœ°æµ‹è¯•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # train_dataset = DummyConversationDataset(config['test_samples'] * 4, config['history_len'], config['num_classes'])
        # test_dataset = DummyConversationDataset(config['test_samples'], config['history_len'], config['num_classes'])

        train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
        test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)

        # --- 2. åˆå§‹åŒ–æ¨¡å‹å’Œ Trainer ---
        model_instance = ModelClass(
            text_dim=TEXT_DIM, 
            speech_dim=SPEECH_DIM, 
            hidden_size=config['hidden_size'], 
            num_classes=config['num_classes']
        )
        
        total_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
        params_in_millions = total_params / 1_000_000
        # print(f"Model Parameters: {params_in_millions:.2f} M") # ç®€åŒ–æ—¥å¿—è¾“å‡º
        
        trainer = Trainer(
            model=model_instance, 
            learning_rate=config['learning_rate'], 
            weight_decay=config['weight_decay'], 
            num_classes=config['num_classes'],
            patience=config['patience'] 
        )
        
        # ğŸš¨ ä¿®æ­£ 5ï¼šæ—©åœåæœªé‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ (å¯é€‰ï¼Œä½†æ¨è)
        # æ¯æ¬¡å¼€å§‹è®­ç»ƒå‰ï¼Œé‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
        trainer.optimizer = AdamW(
            trainer.model.parameters(), 
            lr=config['learning_rate'], 
            weight_decay=config['weight_decay']
        )

        best_f1 = -1.0
        epochs_no_improve = 0
        best_model_state = None
        best_epoch = 0
        best_loss = 0.0
        best_uar = 0.0
        test_labels_at_best = []
        test_preds_at_best = []
        test_gates_at_best = []


        # --- 3. è®­ç»ƒå¾ªç¯ (å¸¦æ—©åœ) ---
        for epoch in range(config['epochs']):
            train_loss, train_f1 = trainer.train_epoch(train_dataloader, epoch)
            
            test_loss, test_f1, test_uar, test_labels, test_preds, test_gates = trainer.evaluate(test_dataloader, desc="Test/Validation")
            
            # ğŸš¨ ä¿®æ­£ 10ï¼šä½¿ç”¨è¡¨æ ¼åŒ–æ—¥å¿—è¾“å‡º
            print(f"[Epoch {epoch+1:02d}] | TrainLoss={train_loss:.4f} | TestLoss={test_loss:.4f} | F1={test_f1:.4f} | UAR={test_uar:.4f}")

            # --- 4. æ—©åœå’Œæ¨¡å‹ä¿å­˜ (åŸºäº UAR) ---
            if test_uar > best_uar:
                best_uar = test_uar
                best_f1 = test_f1
                best_loss = test_loss
                # deepcopy æ¨¡å‹çŠ¶æ€
                best_model_state = copy.deepcopy(model_instance.state_dict())
                epochs_no_improve = 0
                best_epoch = epoch + 1
                
                # æ”¶é›†æœ€ä½³ UAR æ—¶çš„åŸå§‹æ•°æ®
                test_labels_at_best = test_labels
                test_preds_at_best = test_preds
                test_gates_at_best = test_gates
            else:
                epochs_no_improve += 1
            
            if epochs_no_improve >= config['patience']:
                print(f"Early stopping triggered after {best_epoch} epochs (patience={config['patience']}).")
                break
        
        # --- 5. è®°å½•æœ€ç»ˆç»“æœ ---
        train_duration = time.time() - start_time # ğŸš¨ ä¿®æ­£ 4ï¼šè®°å½•å½“å‰ fold çš„æ€»è€—æ—¶
        
        all_test_f1s.append(best_f1)
        
        # å°†æ•°æ®æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨ (ç”¨äºæ±‡æ€»æ‰€æœ‰ Fold çš„æ•°æ®)
        global_labels.extend(test_labels_at_best)
        global_preds.extend(test_preds_at_best)
        global_gate_weights.extend(test_gates_at_best)

        # å°†ç»“æœæ·»åŠ åˆ° DataFrame
        new_row = pd.Series({
            'Session': target_session,
            'Test_Loss': best_loss, 
            'Test_Macro_F1': best_f1,
            'Test_UAR': best_uar, 
            'Train_Time_s': train_duration,
            'Best_Epoch': best_epoch,
            'Params (M)': params_in_millions
        })
        results_df.loc[len(results_df)] = new_row
            
    print("\n=======================================================")
    print(f"| Cross-Validation FINISHED for {ModelClass.__name__} |")
    print("=======================================================")
    
    avg_f1 = results_df['Test_Macro_F1'].mean()
    std_f1 = results_df['Test_Macro_F1'].std()
    
    print(f"Average Macro F1: {avg_f1:.4f} (+/- {std_f1:.4f})")
    
    # è¿”å›ç»“æœ DataFrame å’ŒåŸå§‹æ•°æ® (ä¾› Cell 7 ç”¨äºå›¾è¡¨ç”Ÿæˆ)
    return results_df, global_labels, global_preds, global_gate_weights


def run_experiment(config):
    # ... (ModelClass æ˜ å°„ä¿æŒä¸å˜) ...
    model_map = {
        "GM-EPC": GatedMultimodalEPC,
        "Text-Only": TextOnlyModel,
        "Speech-Only": SpeechOnlyModel,
        "Static-Fusion": StaticFusionModel,
        "Dynamic-WavLM": BaseWavLMModel 
    }
    
    if config['model_name'] not in model_map:
        raise ValueError(f"Unknown model name: {config['model_name']}. Choose from {list(model_map.keys())}")
        
    ModelClass = model_map[config['model_name']]
    
    final_results = run_cross_validation(ModelClass, config) 
    
    return final_results

# ====================================================================
# æœ¬åœ°æµ‹è¯•ä»£ç å— (if __name__ == '__main__':)
# ====================================================================
if __name__ == '__main__':
    print("--- Starting Local Code Logic Test (Full CV Simulation) ---")

    # --- 1. å®éªŒé…ç½® (ä»£æ›¿ YAML æ–‡ä»¶) ---
    CONFIG = {
        'model_name': 'GM-EPC', # æµ‹è¯• Gated Multimodal EPC
        'data_root': '/path/to/iemocap', 
        'history_len': 3,
        'num_classes': 4,
        'batch_size': 8,
        'epochs': 10,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'hidden_size': 64,
        'test_samples': 20, 
        'patience': 3,
        # ğŸš¨ ä¿®æ­£ï¼šæ·»åŠ  feature_cache_path é”®
        'feature_cache_path': './GM-EPC/data/features_cache', 
        # æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿ run_cross_validation ä½¿ç”¨çš„æ˜¯ 'data_root' ä½œä¸ºåŸå§‹è·¯å¾„
    }
    
    # ä¿®æ­£ run_cross_validation ä¸­çš„è·¯å¾„æ˜ å°„ (ç¡®ä¿ä½¿ç”¨ data_root)
    CONFIG['original_data_root'] = CONFIG['data_root']
    
    # --- 2. è¿è¡Œå®Œæ•´çš„è™šæ‹Ÿå®éªŒ ---
    try:
        results_df = run_experiment(CONFIG)
        
        print("\n--- Full CV Simulation Successful ---")
        print("Aggregated Results:")
        print(results_df)
        print(f"\nOverall Mean F1: {results_df['Test_Macro_F1'].mean():.4f} (+/- {results_df['Test_Macro_F1'].std():.4f})")
        print("Training logic and CV loop verified. You are ready for real data!")
        
    except Exception as e:
        print("\n--- Test FAILED ---")
        print(f"An error occurred during the CV loop: {e}")
        # æ‰“å°è¯¦ç»† traceback
        import traceback
        traceback.print_exc()
