# src/features.py (æœ€ç»ˆç‰ˆæœ¬ï¼šæ”¯æŒ mode å‚æ•°æ§åˆ¶åŠ è½½)

import torch
from transformers import AutoModel as TransformersAutoModel, AutoTokenizer, AutoFeatureExtractor
import torchaudio
from funasr import AutoModel
import sys, os
from contextlib import contextmanager
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# --- ç‰¹å¾ç»´åº¦å¸¸é‡ & æ¨¡å‹ ID ---
TEXT_DIM = 768 
SPEECH_DIM = 768 
EMOTION2VEC_MODEL_ID = "iic/emotion2vec_plus_base"
WAVLM_MODEL_ID = "microsoft/wavlm-base-plus"



# --- å…¨å±€æ¨¡å‹å®ä¾‹ ---
global_models ={
    'text_model': None,
    'e2v_model': None,                      
    'wavlm_model': None,                   
    'wavlm_feature_extractor': None,       
    'tokenizer': None,
    'device': torch.device("cpu")
}

@contextmanager
def suppress_funasr_output():
    """Temporarily suppress FunASR model's stderr output."""
    original_stderr = sys.stderr
    sys.stderr = open(os.devnull, 'w')
    try:
        yield
    finally:
        sys.stderr.close()
        sys.stderr = original_stderr

# ã€æ ¸å¿ƒä¿®æ”¹ 1ã€‘ï¼šload_feature_extractors æ¥å— mode å‚æ•°ï¼Œå¹¶åªåŠ è½½æ‰€éœ€çš„æ¨¡å‹
def load_feature_extractors(device, mode="all"):
    """
    æ ¹æ® mode å‚æ•°åŠ è½½æ‰€éœ€çš„ç‰¹å¾æå–å™¨ã€‚
    mode å¯é€‰: 'text', 'e2v', 'wavlm', 'all'
    """
    print(f"Loading feature extractors to device: {device} in mode: {mode}...")

    # --- 0. å¸è½½æ‰€æœ‰æ—§æ¨¡å‹ (é‡è¦: é‡Šæ”¾å†…å­˜) ---
    global_models['text_model'] = None
    global_models['e2v_model'] = None
    global_models['wavlm_model'] = None
    global_models['wavlm_feature_extractor'] = None
    
    # 1. BERT (Text)
    if mode in ["all", "text"]:
        MODEL_NAME = "bert-base-uncased"
        global_models['tokenizer'] = AutoTokenizer.from_pretrained(MODEL_NAME)
        global_models['text_model'] = TransformersAutoModel.from_pretrained(MODEL_NAME).to(device)
        print("âœ… BERT Text Model loaded.")

    # 2. Emotion2vec (e2v)
    if mode in ["all", "e2v"]:
        try:
            global_models['e2v_model'] = AutoModel(model=EMOTION2VEC_MODEL_ID)
            print(f"âœ… emotion2vec model loaded: {EMOTION2VEC_MODEL_ID}")
        except Exception as e:
            print(f"âš ï¸ Warning: E2V failed to load. {e}")
            
    # 3. WavLM
    if mode in ["all", "wavlm"]:
        try:
            global_models['wavlm_feature_extractor'] = AutoFeatureExtractor.from_pretrained(WAVLM_MODEL_ID)
            global_models['wavlm_model'] = TransformersAutoModel.from_pretrained(WAVLM_MODEL_ID).to(device)
            print(f"âœ… WavLM model loaded: {WAVLM_MODEL_ID}")
        except Exception as e:
            print(f"âš ï¸ Warning: WavLM failed to load. {e}")
    
    global_models['device'] = device 
    print(f"âœ… Current active models: T:{bool(global_models['text_model'])}, E2V:{bool(global_models['e2v_model'])}, WLM:{bool(global_models['wavlm_model'])}")


    # === éªŒè¯æ¨¡å‹ç»´åº¦ ===
    actual_text_dim = global_models['text_model'].config.hidden_size
    if actual_text_dim != TEXT_DIM:
        print(f"âš ï¸ è­¦å‘Šï¼šTEXT_DIM å¸¸é‡ ({TEXT_DIM}) ä¸å®é™…æ¨¡å‹ç»´åº¦ ({actual_text_dim}) ä¸åŒ¹é…ã€‚è¯·ä¿®æ­£ TEXT_DIMã€‚")

    print(f"Feature extractors loading process finished. Be aware of potential OOM issues when running all models on GPU.")

# ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘ï¼šextract_single_feature è¿”å›ä¸‰ä¸ªç‰¹å¾åºåˆ—
def extract_single_feature(text_list, audio_path_list):
    """
    æå–å•ä¸ªå›åˆçš„ç‰¹å¾ï¼Œä¸æ´»åŠ¨çš„æ¨¡å‹è¿”å›é›¶å‘é‡å ä½ç¬¦ã€‚
    è¿”å› F_t, F_s_e2v, F_s_wavlm
    """
    device = global_models['device']
    text_model = global_models['text_model']
    e2v_model = global_models['e2v_model']
    wavlm_model = global_models['wavlm_model']
    wavlm_extractor = global_models['wavlm_feature_extractor']
    tokenizer = global_models['tokenizer']
    
    # åˆå§‹åŒ–ä¸‰ä¸ªç‰¹å¾åˆ—è¡¨
    F_t_list = []
    F_s_e2v_list = []
    F_s_wavlm_list = []

    for text, audio_path in zip(text_list, audio_path_list):
        
        # --- 1. æ–‡æœ¬ç‰¹å¾æå– (F_t) ---
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
        
        with torch.no_grad():
            outputs = text_model(**inputs) 
            text_feature = outputs.last_hidden_state[:, 0, :].squeeze(0) # [D_t]
        
        F_t_list.append(text_feature)

        # --- 2. è¯­éŸ³ç‰¹å¾æå– (Emotion2vec) ---
        if e2v_model is not None:
            try:
                with torch.no_grad():
                    with suppress_funasr_output():
                        res = e2v_model.generate(input=audio_path, granularity="utterance", extract_embedding=True)
                    
                    if isinstance(res, list) and res and 'feats' in res[0]:
                        e2v_feature_np = res[0]['feats']
                        e2v_feature = torch.from_numpy(e2v_feature_np).float().to(device).squeeze() 
                        
                        if e2v_feature.shape[-1] != SPEECH_DIM:
                            print(f"âš ï¸ Warning: E2V dim mismatch for {audio_path}. ({e2v_feature.shape[-1]})")
                        F_s_e2v_list.append(e2v_feature)
                    else:
                        raise RuntimeError("FunASR generate did not return expected feature format or 'feats' key.")
            except Exception as e:
                print(f"Error processing audio {audio_path} using E2V: {e}. Returning zero vector.")
                F_s_e2v_list.append(torch.zeros(SPEECH_DIM, device=device)) 
        else:
             F_s_e2v_list.append(torch.zeros(SPEECH_DIM, device=device)) 


        # --- 3. è¯­éŸ³ç‰¹å¾æå– (WavLM) ---
        if wavlm_model is not None:
            try:
                # å¿…é¡»ä½¿ç”¨ torchaudio åŠ è½½åŸå§‹éŸ³é¢‘
                waveform, sr = torchaudio.load(audio_path)
                # WavLM è¦æ±‚ 16k Hz
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000).to(device)(waveform)
                
                # é¢„å¤„ç†
                inputs_wavlm = wavlm_extractor(waveform.squeeze(0), return_tensors="pt", sampling_rate=16000)
                inputs_wavlm = {k: v.to(device) for k, v in inputs_wavlm.items()}
                
                with torch.no_grad():
                    outputs_wavlm = wavlm_model(**inputs_wavlm)
                    # WavLM ç‰¹å¾æå–ï¼šå–æ‰€æœ‰æ—¶é—´æ­¥çš„å¹³å‡å€¼ (Mean Pooling) ä½œä¸º utterance-level ç‰¹å¾
                    wavlm_feature = outputs_wavlm.last_hidden_state.mean(dim=1).squeeze(0) # [D_s]
                
                F_s_wavlm_list.append(wavlm_feature)
            except Exception as e:
                print(f"âš ï¸ Warning: WavLM feature extraction failed for {audio_path}. {e}. Returning zero vector.")
                F_s_wavlm_list.append(torch.zeros(SPEECH_DIM, device=device))
        else:
             F_s_wavlm_list.append(torch.zeros(SPEECH_DIM, device=device))


    # å°† L ä¸ªå›åˆçš„ç‰¹å¾å †å 
    F_t_sequence = torch.stack(F_t_list, dim=0)          # [L, D_t]
    F_s_emotion2vec_sequence = torch.stack(F_s_e2v_list, dim=0) # [L, D_s]
    F_s_wavlm_sequence = torch.stack(F_s_wavlm_list, dim=0)     # [L, D_s]
    
    # è¿”å›ä¸‰ä¸ªç‰¹å¾åºåˆ—

    return F_t_sequence, F_s_emotion2vec_sequence, F_s_wavlm_sequence

# ----------------------------------------------------------------------
# è™šæ‹Ÿæ•°æ®ç”Ÿæˆå‡½æ•° (ç”¨äºæœ¬åœ° model.py å’Œ trainer.py çš„è°ƒè¯•)
# ... (ä¿æŒä¸å˜) ...

def get_dummy_features(batch_size, sequence_length):
    """
    è¿”å›éšæœºç”Ÿæˆçš„ç‰¹å¾å¼ é‡ï¼Œæ¨¡æ‹ŸçœŸæ­£çš„ç‰¹å¾æå–å™¨è¾“å‡ºã€‚
    """
    # æ¨¡æ‹Ÿ BERT ç‰¹å¾ (F_t)
    F_t = torch.randn(batch_size, sequence_length, TEXT_DIM) 
    # æ¨¡æ‹Ÿ Emotion2vec ç‰¹å¾ (F_s_e2v)
    F_s_e2v = torch.randn(batch_size, sequence_length, SPEECH_DIM) 
    # æ¨¡æ‹Ÿ WavLM ç‰¹å¾ (F_s_wavlm)
    F_s_wavlm = torch.randn(batch_size, sequence_length, SPEECH_DIM) 
    
    # ğŸš¨ ä¿®æ­£ï¼šè¿”å›ä¸‰ä¸ªå¼ é‡
    return F_t, F_s_e2v, F_s_wavlm

def get_dummy_labels(batch_size, num_classes):
    """
    è¿”å›éšæœºç”Ÿæˆçš„æ•´æ•°æ ‡ç­¾ã€‚
    """
    labels = torch.randint(0, num_classes, (batch_size,)) 
    return labels

# ... (å…¶ä½™ä»£ç ä¿æŒä¸å˜) ...

if __name__ == '__main__':
    print("--- Testing feature extractor loading (Mode Check) ---")
    device = torch.device("cpu")
    
    # Test 1: Only Text mode
    print("\n[TEST 1] Mode: 'text' (Should load only BERT)")
    try:
        load_feature_extractors(device, mode='text')
        assert global_models['text_model'] is not None
        assert global_models['e2v_model'] is None
        assert global_models['wavlm_model'] is None
        print("âœ… Test 1 Passed.")
    except Exception as e:
        print(f"âŒ Test 1 FAILED: {e}")

    # Test 2: Only E2V mode
    print("\n[TEST 2] Mode: 'e2v' (Should load only E2V)")
    try:
        # æ­¤æ—¶ global_models['text_model'] ä¼šè¢«å¸è½½ (è®¾ç½®ä¸º None)
        load_feature_extractors(device, mode='e2v')
        assert global_models['text_model'] is None
        assert global_models['e2v_model'] is not None
        assert global_models['wavlm_model'] is None
        print("âœ… Test 2 Passed.")
    except Exception as e:
        print(f"âŒ Test 2 FAILED: {e}")
        
    # Test 3: All mode (Warning: Memory heavy)
    print("\n[TEST 3] Mode: 'all' (Should load all three)")
    try:
        load_feature_extractors(device, mode='all')
        assert global_models['text_model'] is not None
        assert global_models['e2v_model'] is not None
        assert global_models['wavlm_model'] is not None
        print("âœ… Test 3 Passed.")
    except Exception as e:
        print(f"âŒ Test 3 FAILED: {e}")