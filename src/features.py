import torch
# 1. å¯¼å…¥ transformers çš„ AutoModel å¹¶å‘½åä¸º TransformersAutoModel æˆ– TextAutoModel
from transformers import AutoModel as TransformersAutoModel, AutoTokenizer

# 2. å¯¼å…¥ funasr çš„ AutoModelï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨ AutoModelï¼Œæˆ–è€…å‘½åä¸º FunASRAutoModel æˆ– SpeechAutoModel
from funasr import AutoModel
import numpy as np 
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


# --- ç‰¹å¾ç»´åº¦å¸¸é‡ ---
TEXT_DIM = 768 
SPEECH_DIM = 768 # å·²æ ¹æ®ä¹‹å‰çš„è°ƒè¯•ç»“æœä¿®æ­£ä¸º 768

# --- å…¨å±€æ¨¡å‹å®ä¾‹ ---
global_models ={
    'text_model': None,
    'speech_model': None,
    'tokenizer': None,
    'device': torch.device("cpu") # é»˜è®¤ä¸º CPUï¼Œåœ¨ load_feature_extractors ä¸­ä¼šè¢«æ›´æ–°
}


def load_feature_extractors(device):
    """
    åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ (BERT å’Œ emotion2vec)ã€‚
    """
    print(f"Loading feature extractors to device: {device}...")

    # 1. æ–‡æœ¬ç‰¹å¾æå–å™¨ (BERT Base Uncased)
    MODEL_NAME = "bert-base-uncased"

    global_models['tokenizer'] = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=False,
        revision="main",
        token=None
    )
    global_models['text_model'] = TransformersAutoModel.from_pretrained(
        MODEL_NAME,
        trust_remote_code=False,
        revision="main",
        token=None
    ).to(device)
    
    # 2. è¯­éŸ³ç‰¹å¾æå–å™¨ (emotion2vec)
    EMOTION2VEC_MODEL_ID = "iic/emotion2vec_plus_base"

    try:
        global_models['speech_model'] = AutoModel(model=EMOTION2VEC_MODEL_ID)
        print(f"âœ… emotion2vec model loaded: {EMOTION2VEC_MODEL_ID}")

    except Exception as e:
        raise RuntimeError(f"Failed to load emotion2vec model {EMOTION2VEC_MODEL_ID}. The specific error is: {e}")

    # === å…³é”®ä¿®æ­£ 1ï¼šæ›´æ–°å…¨å±€è®¾å¤‡çŠ¶æ€ ===
    global_models['device'] = device 
    print(f"âœ… Global device state updated to: {global_models['device']}") # ä¿ç•™è¿™æ¡æ‰“å°ï¼Œç¡®ä¿è®¾å¤‡çŠ¶æ€æ›´æ–°æˆåŠŸ

    # === éªŒè¯æ¨¡å‹ç»´åº¦ ===
    actual_text_dim = global_models['text_model'].config.hidden_size
    print(f"âœ… Text Model loaded. Configured dim: {TEXT_DIM}, Actual dim: {actual_text_dim}")
    if actual_text_dim != TEXT_DIM:
        print(f"âš ï¸ è­¦å‘Šï¼šTEXT_DIM å¸¸é‡ ({TEXT_DIM}) ä¸å®é™…æ¨¡å‹ç»´åº¦ ({actual_text_dim}) ä¸åŒ¹é…ã€‚è¯·ä¿®æ­£ TEXT_DIMã€‚")

    print(f"âœ… Speech Model loaded. Configured dim: {SPEECH_DIM}") 

    print("Feature extractors loaded successfully.")


def extract_single_feature(text_list, audio_path_list):
    """
    æå–å•ä¸ªå¯¹è¯æ ·æœ¬ï¼ˆLä¸ªå›åˆï¼‰çš„ç‰¹å¾ã€‚
    """
    device = global_models['device']
    text_model = global_models['text_model']
    speech_model = global_models['speech_model']
    tokenizer = global_models['tokenizer']
    
    # [å·²ç§»é™¤] è°ƒè¯•ä¿¡æ¯ 1ï¼šç¡®è®¤å½“å‰ä½¿ç”¨çš„è®¾å¤‡
    
    # åˆå§‹åŒ–ç‰¹å¾åˆ—è¡¨
    F_t_list = []
    F_s_list = []

    for text, audio_path in zip(text_list, audio_path_list):
        
        # --- 1. æ–‡æœ¬ç‰¹å¾æå– (F_t) ---
        inputs = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )

        # === å…³é”®ä¿®æ­£ 2ï¼šä½¿ç”¨å­—å…¸éå†å’Œè§£åŒ… ===
        # ç¡®ä¿ inputs å­—å…¸ä¸­çš„æ‰€æœ‰å¼ é‡éƒ½ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ (device)
        inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
        
        # [å·²ç§»é™¤] è°ƒè¯•ä¿¡æ¯ 2ï¼šæ£€æŸ¥è¾“å…¥å¼ é‡çš„è®¾å¤‡
            
        # æå–ç‰¹å¾
        with torch.no_grad():
            # ä½¿ç”¨å­—å…¸è§£åŒ…ä¼ å…¥æ‰€æœ‰åœ¨ GPU ä¸Šçš„å¼ é‡
            outputs = text_model(**inputs) 
            
            # ğŸš¨ ä¿®æ­£ï¼šæ–°å¢ç‰¹å¾èµ‹å€¼è¡Œ
            text_feature = outputs.last_hidden_state[:, 0, :].squeeze(0) # (D_t)

        
        F_t_list.append(text_feature)

        # --- 2. è¯­éŸ³ç‰¹å¾æå– (F_s) ---
        
        try:
            # æå–ç‰¹å¾ï¼šä½¿ç”¨ FunASR æ¨¡å‹çš„ generate æ¥å£
            with torch.no_grad():
                res = speech_model.generate(
                    input=audio_path,
                    granularity="utterance",
                    extract_embedding=True,
                    progress_bar=False,   # âœ… ç¦æ­¢ tqdm è¿›åº¦æ¡
                    show_progress=False,  # âœ… ä¸€äº›ç‰ˆæœ¬ç”¨è¿™ä¸ªå‚æ•°
                    verbose=False         # âœ… æœ‰çš„ç‰ˆæœ¬è¿˜è¦åŠ è¿™ä¸ªé˜²æ­¢æ‰“å°
                )

                if isinstance(res, list) and res and 'feats' in res[0]:
                    speech_feature_np = res[0]['feats']
                    
                    # ç¡®ä¿è½¬æ¢ä¸º Tensor åï¼Œå‘é€åˆ° DEVICE
                    speech_feature = torch.from_numpy(speech_feature_np).float().to(device).squeeze() 
                    
                    # è¿è¡Œæ—¶éªŒè¯ï¼š
                    if speech_feature.shape[-1] != SPEECH_DIM:
                        # ä¿ç•™è¿™ä¸ªè­¦å‘Šï¼Œå› ä¸ºå®ƒæ£€æŸ¥çš„æ˜¯æ•°æ®æœ¬èº«çš„å®Œæ•´æ€§ï¼Œéå¸¸é‡è¦ã€‚
                        print(f"âš ï¸ è¿è¡Œæ—¶è­¦å‘Šï¼šéŸ³é¢‘æ–‡ä»¶ {audio_path} å®é™…è¯­éŸ³ç»´åº¦ ({speech_feature.shape[-1]}) ä¸ SPEECH_DIM ({SPEECH_DIM}) ä¸åŒ¹é…ï¼")

                    F_s_list.append(speech_feature)
                else:
                    raise RuntimeError("FunASR generate did not return expected feature format or 'feats' key.")

        except Exception as e:
            print(f"Error loading or processing audio {audio_path} using FunASR: {e}. Returning zero vector.")
            # ç¡®ä¿é›¶å‘é‡å ä½ç¬¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            F_s_list.append(torch.zeros(SPEECH_DIM, device=device)) 
            
            
    # å°† L ä¸ªå›åˆçš„ç‰¹å¾å †å 
    F_t_sequence = torch.stack(F_t_list, dim=0) # [L, D_t]
    F_s_sequence = torch.stack(F_s_list, dim=0) # [L, D_s]
    
    # ğŸš¨ æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„å¼ é‡ç°åœ¨å°†ç•™åœ¨ GPU ä¸Š
    return F_t_sequence, F_s_sequence 


# ----------------------------------------------------------------------
# è™šæ‹Ÿæ•°æ®ç”Ÿæˆå‡½æ•° (ç”¨äºæœ¬åœ° model.py å’Œ trainer.py çš„è°ƒè¯•)
# ... (ä¿æŒä¸å˜) ...

def get_dummy_features(batch_size, sequence_length):
    """
    è¿”å›éšæœºç”Ÿæˆçš„ç‰¹å¾å¼ é‡ï¼Œæ¨¡æ‹ŸçœŸæ­£çš„ç‰¹å¾æå–å™¨è¾“å‡ºã€‚
    """
    # å‡è®¾åœ¨ CPU ä¸Šç”Ÿæˆè™šæ‹Ÿæ•°æ®ï¼Œä½†åœ¨å®é™…è®­ç»ƒä¸­éœ€è¦ .to(device)
    F_t = torch.randn(batch_size, sequence_length, TEXT_DIM) 
    F_s = torch.randn(batch_size, sequence_length, SPEECH_DIM) 
    return F_t, F_s

def get_dummy_labels(batch_size, num_classes):
    """
    è¿”å›éšæœºç”Ÿæˆçš„æ•´æ•°æ ‡ç­¾ã€‚
    """
    labels = torch.randint(0, num_classes, (batch_size,)) 
    return labels

if __name__ == '__main__':
    print("Testing feature extractor loading...")
    try:
        load_feature_extractors(torch.device("cpu"))
        print("Model loading test completed.") 
    except Exception as e:
        print(f"Model loading FAILED: {e}")