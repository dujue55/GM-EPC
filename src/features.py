import torch
# 1. å¯¼å…¥ transformers çš„ AutoModel å¹¶å‘½åä¸º TransformersAutoModel æˆ– TextAutoModel
from transformers import AutoModel as TransformersAutoModel, AutoTokenizer

# 2. å¯¼å…¥ funasr çš„ AutoModelï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨ AutoModelï¼Œæˆ–è€…å‘½åä¸º FunASRAutoModel æˆ– SpeechAutoModel
from funasr import AutoModel
import numpy as np # éœ€è¦ numpy æ¥å¤„ç† funasr çš„è¾“å‡º


# --- ç‰¹å¾ç»´åº¦å¸¸é‡ ---
TEXT_DIM = 768 # å‡è®¾ä½¿ç”¨ BERT Baseï¼Œå…¶è¾“å‡ºç»´åº¦ä¸º 768 (D_t)
SPEECH_DIM = 1024 # å‡è®¾ emotion2vec ä½¿ç”¨ 1024 ç»´åº¦ (D_s) 

# --- å…¨å±€æ¨¡å‹å®ä¾‹ ---
# ... (ä¿æŒä¸å˜)

global_models ={
    'text_model': None,
    'speech_model': None,
    'tokenizer': None,
    'device': torch.device("cpu") # é»˜è®¤ä¸º CPUï¼Œåœ¨ run_experiment ä¸­ä¼šè¢«æ›´æ–°
}


def load_feature_extractors(device):
    """
    åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ (BERT å’Œ emotion2vec)ã€‚
    """
    print(f"Loading feature extractors to device: {device}...")

    # 1. æ–‡æœ¬ç‰¹å¾æå–å™¨ (BERT Base Uncased)
    MODEL_NAME = "bert-base-uncased"  # ä½¿ç”¨æœ€åŸå§‹åç§°

    global_models['tokenizer'] = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=False,
        revision="main",
        token=None
    )
    global_models['text_model'] = TransformersAutoModel.from_pretrained( # <--- ä¿®æ­£!
        MODEL_NAME,
        trust_remote_code=False,
        revision="main",
        token=None
    ).to(device)
    
    # 2. è¯­éŸ³ç‰¹å¾æå–å™¨ (emotion2vec)
    EMOTION2VEC_MODEL_ID = "iic/emotion2vec_plus_base" 

    try:
        global_models['speech_model'] = AutoModel(model=EMOTION2VEC_MODEL_ID)
        print(f"âœ… emotion2vec model loaded: {EMOTION2VEC_MODEL_ID}")

    except Exception as e:
        raise RuntimeError(f"Failed to load emotion2vec model {EMOTION2VEC_MODEL_ID}. The specific error is: {e}")

    # === éªŒè¯æ¨¡å‹ç»´åº¦ ===
    actual_text_dim = global_models['text_model'].config.hidden_size
    print(f"âœ… Text Model loaded. Configured dim: {TEXT_DIM}, Actual dim: {actual_text_dim}")
    if actual_text_dim != TEXT_DIM:
        print(f"âš ï¸ è­¦å‘Šï¼šTEXT_DIM å¸¸é‡ ({TEXT_DIM}) ä¸å®é™…æ¨¡å‹ç»´åº¦ ({actual_text_dim}) ä¸åŒ¹é…ã€‚è¯·ä¿®æ­£ TEXT_DIMã€‚")

    print(f"âœ… Speech Model loaded. Configured dim: {SPEECH_DIM}") 

    print("Feature extractors loaded successfully.")


def extract_single_feature(text_list, audio_path_list):
    """
    æå–å•ä¸ªå¯¹è¯æ ·æœ¬ï¼ˆLä¸ªå›åˆï¼‰çš„ç‰¹å¾ã€‚
    """
    device = global_models['device']
    text_model = global_models['text_model']
    speech_model = global_models['speech_model']
    tokenizer = global_models['tokenizer']
    
    # åˆå§‹åŒ–ç‰¹å¾åˆ—è¡¨
    F_t_list = []
    F_s_list = []

    for text, audio_path in zip(text_list, audio_path_list):
        
        # --- 1. æ–‡æœ¬ç‰¹å¾æå– (F_t) ---
        inputs = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )

        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„è¾“å…¥å¼ é‡éƒ½è¢«ç§»åŠ¨åˆ° GPU (åŒ…æ‹¬å¯èƒ½ç¼ºå¤±çš„ token_type_ids/position_ids)
        # æˆ‘ä»¬å°†å®ƒä»¬ç§»åŠ¨åˆ° GPUï¼Œå¹¶ç¡®ä¿è¾“å…¥ä¸­ä¸åŒ…å«ä¸éœ€è¦çš„ CPU å¼ é‡
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        token_type_ids = inputs.get('token_type_ids', torch.zeros_like(input_ids)).to(device) # ç¡®ä¿ token_type_ids å­˜åœ¨ä¸”åœ¨ GPU

        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = text_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids 
            )
            
            # ğŸš¨ ä¿®æ­£ï¼šæ–°å¢ç‰¹å¾èµ‹å€¼è¡Œ
            text_feature = outputs.last_hidden_state[:, 0, :].squeeze(0) # (D_t)

        
        F_t_list.append(text_feature) # ç°åœ¨ text_feature å·²å®šä¹‰

        # --- 2. è¯­éŸ³ç‰¹å¾æå– (F_s) ---
        
        try:
            # æå–ç‰¹å¾ï¼šä½¿ç”¨ FunASR æ¨¡å‹çš„ generate æ¥å£
            with torch.no_grad():
                res = speech_model.generate(
                    input=audio_path, 
                    granularity="utterance", 
                    extract_embedding=True 
                )

                if isinstance(res, list) and res and 'feats' in res[0]:
                    speech_feature_np = res[0]['feats']
                    
                    # ğŸš¨ ä¿®æ­£ 2ï¼šç¡®ä¿è½¬æ¢ä¸º Tensor åï¼Œå‘é€åˆ° DEVICE
                    # ç§»é™¤ .cpu()ï¼Œå¹¶ä½¿ç”¨ .to(device) æ˜ç¡®å‘é€åˆ°æ­£ç¡®çš„è®¾å¤‡
                    speech_feature = torch.from_numpy(speech_feature_np).float().to(device).squeeze() 
                    
                    # è¿è¡Œæ—¶éªŒè¯ï¼š
                    if speech_feature.shape[-1] != SPEECH_DIM:
                        print(f"âš ï¸ è¿è¡Œæ—¶è­¦å‘Šï¼šéŸ³é¢‘æ–‡ä»¶ {audio_path} å®é™…è¯­éŸ³ç»´åº¦ ({speech_feature.shape[-1]}) ä¸ SPEECH_DIM ({SPEECH_DIM}) ä¸åŒ¹é…ï¼")

                    F_s_list.append(speech_feature)
                else:
                    raise RuntimeError("FunASR generate did not return expected feature format or 'feats' key.")

        except Exception as e:
            print(f"Error loading or processing audio {audio_path} using FunASR: {e}. Returning zero vector.")
            # ğŸš¨ ä¿®æ­£ 3ï¼šç¡®ä¿é›¶å‘é‡å ä½ç¬¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            F_s_list.append(torch.zeros(SPEECH_DIM, device=device)) 
            
            
    # å°† L ä¸ªå›åˆçš„ç‰¹å¾å †å 
    F_t_sequence = torch.stack(F_t_list, dim=0) # [L, D_t]
    F_s_sequence = torch.stack(F_s_list, dim=0) # [L, D_s]
    
    # ğŸš¨ æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„å¼ é‡ç°åœ¨å°†ç•™åœ¨ GPU ä¸Šï¼Œä»è€Œè§£å†³ Runtime Error

    global_models['device'] = device # <--- ä¿®æ­£! å°†å…¨å±€è®¾å¤‡æ›´æ–°ä¸ºå®é™…ä¼ å…¥çš„è®¾å¤‡
    
    return F_t_sequence, F_s_sequence 


# ----------------------------------------------------------------------
# è™šæ‹Ÿæ•°æ®ç”Ÿæˆå‡½æ•° (ç”¨äºæœ¬åœ° model.py å’Œ trainer.py çš„è°ƒè¯•)
def get_dummy_features(batch_size, sequence_length):
    """
    è¿”å›éšæœºç”Ÿæˆçš„ç‰¹å¾å¼ é‡ï¼Œæ¨¡æ‹ŸçœŸæ­£çš„ç‰¹å¾æå–å™¨è¾“å‡ºã€‚
    """
    # å‡è®¾åœ¨ CPU ä¸Šç”Ÿæˆè™šæ‹Ÿæ•°æ®ï¼Œä½†åœ¨å®é™…è®­ç»ƒä¸­éœ€è¦ .to(device)
    F_t = torch.randn(batch_size, sequence_length, TEXT_DIM) 
    F_s = torch.randn(batch_size, sequence_length, SPEECH_DIM) 
    return F_t, F_s

def get_dummy_labels(batch_size, num_classes):
    """
    è¿”å›éšæœºç”Ÿæˆçš„æ•´æ•°æ ‡ç­¾ã€‚
    """
    labels = torch.randint(0, num_classes, (batch_size,)) 
    return labels

if __name__ == '__main__':
    print("Testing feature extractor loading...")
    try:
        load_feature_extractors(torch.device("cpu"))
        print("Model loading test completed.") 
    except Exception as e:
        print(f"Model loading FAILED: {e}")